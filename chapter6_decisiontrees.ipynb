{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Train and fine-tune a Decision Tree for the moons dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[ 0.72421123,  0.54211831],\n",
       "        [-0.32988897,  1.18780642],\n",
       "        [-0.93047904,  0.38015665],\n",
       "        ...,\n",
       "        [ 1.02806188,  0.53426052],\n",
       "        [ 0.70049764,  0.46673958],\n",
       "        [ 0.96015176,  0.39724371]]),\n",
       " array([0, 0, 0, ..., 1, 0, 0]))"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "moons = make_moons(n_samples = 10000, noise= 0.4)\n",
    "moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.46654404,  1.22651313],\n",
       "       [ 0.58662884, -0.18575817],\n",
       "       [ 0.7280187 ,  1.0420609 ],\n",
       "       ...,\n",
       "       [-0.17366043,  0.58585676],\n",
       "       [ 0.86602821,  0.25804901],\n",
       "       [-1.82357241, -0.03429702]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "moons[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets split the dataset in test and train datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(moons[0], moons[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'max_leaf_nodes': [2, 5, 7, 10]})"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#Now we'll do grid search cross validation to look for the best hyperparameters for a decision tree classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid = {'max_leaf_nodes':[2,5,7,10]}\n",
    "DTreeClassifier = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(DTreeClassifier, param_grid)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.37327380477785116"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "y_predicted = grid_search.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_predicted)\n",
    "np.sqrt(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8606666666666667"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_train,y_predicted)"
   ]
  },
  {
   "source": [
    "## Now grow a forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ShuffleSplit(n_splits=1000, random_state=0, test_size=None, train_size=100)\n"
     ]
    }
   ],
   "source": [
    "#We'll generate 1000 subsets first with 100 instances in each:\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "rs = ShuffleSplit(n_splits=1000, train_size=100, random_state=0)\n",
    "print(rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5455 5093 3663 4346 5113\n",
      " 5430 6080 6891 3270 7011 2853 6887   73 3685 4038 4112 2729 6677 3614\n",
      " 4325 4208   26 2360 1768 3828 1654  523  477 2300 3233 2282 5456 2642\n",
      " 4410 3717 5448 5358 5580 3968 2512 7052 1058  358 3682 7161 7008 2132\n",
      " 1650  728 6055 4753 2551 1490  118 2964  993 3376 4700 6440 2133 2089\n",
      " 3967 2914]  Test : [1738 4293 7136 ... 5576 4429 2894]\n",
      "Train : [6480 4902  540 3375 1391 5519  564 2050 2258 4036 5386 3386 6204  179\n",
      "  673 1365 3484 4290 2296 1118 3602 7403 6436 3476 2018 4331 6793 2133\n",
      "  594 5186 2106 4069 5487 4153 1949 4887 3096  929 5503 5339 7450 4801\n",
      " 6381 1777 6543 5675 1561 6116 6447 5286  359 5933 5090 7163 7098 6335\n",
      " 4174 2288 5408 4823  781 1589 3261 1952  818 2679 3201 4157 2815   16\n",
      " 3266 5871 1076  453 4160 3293  178 2898  688 4125 6659 3751 2161 4654\n",
      " 1326  258  180 6867   14 5293   15 7334 2187 1748  353 3741 6938 2719\n",
      " 4704 5860]  Test : [3627 5566  346 ... 5468 2931 6764]\n",
      "Train : [ 770 3508 3501 7362 4271 5181 1173 5550 5435 3321 1269 2855 3245 6856\n",
      "  564 2553 1014  854  912 6633  220  697  296 1317 2315 3349 3916  682\n",
      " 3263 3466  200 1665 1620 4591 1169 2768  593 7499  608 1596 4389 6252\n",
      " 4331 6448 2634  901 5729  739 3801 4349 4085 7160 2013 2360 5881 5658\n",
      " 1558 4275 1088 6500 4855 1983 7035  169 6514 2241 4709 1822 1361 2262\n",
      " 4238 5253 5488  925  522 1025 1699 4825  451 1273 6644 5996 5255 1987\n",
      " 2946 3409   47 5686  864 4520 2183 2671 2291 4420 3032 4633 6108 3275\n",
      " 7122 3258]  Test : [ 732 6839 1339 ... 3040 3524 5615]\n",
      "Train : [3092 2044 2042 4611 4133 1055 1648 5000 1653 2422 6356 3978 3892 4068\n",
      "   48 5187 6298 7242 4482 6297 1913 1660  565 6943 3570 2878 1144 6679\n",
      " 6209 3965 2908  738 6331 6166 2917 5678 2767  267 2329 5850 4319 2148\n",
      "  109 6018 3161 4717 7182  293 1621 6660 4110 3889 6009 1981   55 6460\n",
      " 1032 6951 4857 2924 2783 2282 4010 3372 6673 4868 1500 1387 1308 4948\n",
      " 6174 3802 5446 4641 5653  408 2125 2977 1666 3578  438   25 1354 6921\n",
      "  172 3705 5656  359 6738 3407 2175 6624 5003 1599 6929 1096 6711 6543\n",
      "  287  669]  Test : [1326 4685 3309 ... 4696  427 2666]\n",
      "Train : [ 557 1478 2637 5392 4892 2108  791 2436 1137  598  679 2078 6804 5007\n",
      " 3483   13 2551 6458 6538 3468 3065 5849 5065 3267  539 1718 2478  897\n",
      " 4559 3307 5638 5890 4021 4745 6667 4797 6895 4316 5768 1084 1266 3063\n",
      " 2933 2387 2832 3264 2839 3191 6615 6800 1730 7487 5946 2705  444 3033\n",
      " 3987 6335 4178 3326 1800 4716 4673 7406 6301 5703 5674 5631 6531 6109\n",
      " 2079 6942 3218 2966 6999 6884 1685 4667 1056 3708 2942 4122 1965 7423\n",
      " 4142 4196 5338 5837 1257 2338 7364 6947 4645 5212 7419  902 5189 3303\n",
      " 2276 7411]  Test : [ 420 5272 6836 ... 1211 2179 2793]\n",
      "Train : [ 184 1858   26 5289 3892 3743  387 2259 3790 6765 2798 7036  544 7029\n",
      " 2316 4801 4624 3230 4976 6729  429 3918 7263   27 5559 6669 2801 3365\n",
      " 4978 2175 1221 4915  525 7233  205 7291 5122 4257 5484 6704 5796 6536\n",
      " 1432 7456 5407 2366 2204   28 3334 1974 1483 4274 2337 2725 3110 2298\n",
      " 4621 4898 4584 5817 1126 4295  213   13 6944 6129 6595 3744  156 2794\n",
      " 5424 2826 4632 3733 5685 4393 1293 7497 5981   87 3250 3380 5580 2751\n",
      "  667 3486 1579  515 4565 3442  117 5309 2836 5737 3169  122 5648 6904\n",
      " 4220 3248]  Test : [2215 3987 5408 ...  581  477  330]\n",
      "Train : [1993 7143 2073 5347 2459 5226 7011 2226 1147 4277 6790 4011 2466 5176\n",
      " 5497  140 5292 1215 1839 5773 2280 4955 3765 4399 4704 5205 4537  947\n",
      " 5425 1205 3838 1164 3134 2152  702 6485 3163 3136 1139 5880 5301  769\n",
      " 7243 7292 2505 6246 1335 4813 1895 1395 5495 6121  211 5881 3069 1462\n",
      " 6920 4351 6198 1945 6862  882  786 6313 5994  915 4549 6454  708 6016\n",
      " 2369 6349 6056 5664 3296 2716 6354 2486  133 2766 3602 3709 5079 4136\n",
      " 6684   67 6383 3109 7212 2103  743 5492 3837 2496 5989 6293 4652  657\n",
      " 5140  964]  Test : [  38 3888 5424 ... 5100 6899 1698]\n",
      "Train : [2883 2736 6103  220 5160 2146  698 3838 1066 6615 1186 5411 3230 1171\n",
      " 4604 6012  176  643 3636 1525 5082  260  193 1114 3447 4154  608 2504\n",
      " 7100 6024 6352 7266 3823 4915 6077 3330 5344  809  417 3526 5125 3989\n",
      " 5024 3766 7468 6907 3711 1099 5451 4480 1121 1281 1529 3450 1177 4098\n",
      " 6415 6587 3219 4559  556 2237 7272  538 1400 3107 3541 4564 4099 5761\n",
      " 7044 5480 7224 2851 2971  156 5680 7474 6626 5286 1799 1893  532  451\n",
      " 6199 3615 7456  323 3669  973  700 4752  971 4156 3149 3271 3495 1547\n",
      " 2686 3470]  Test : [4792 1931 4250 ... 1594 5425 4678]\n",
      "Train : [3610 7489 2008 7406 2012 1868 5590  761 7440 1623 2849 5935  818 7154\n",
      " 5344 6533 4423 4548 6332 5170 7137  844 5116 7133 3334 2779  130 3234\n",
      " 6121 1612 5187 5807 6667 2054 5353 2146 3845  526 1394 6863 3981 1710\n",
      " 3378 4972 3237   46 1667  487 6244 4360 6750  140 2198 3505 3673 7218\n",
      " 5882 3843 4530 2070 7394 2988 2617 3283  918 1473 3595 5281 4434 5976\n",
      " 5546 4148 3207 2082 2915 6972 2974  353 1320 4032 4115  935 3783 6965\n",
      " 5643 3966 3005 6846 1033 2564 3986  592  662 2277 2473 5651 2041 7232\n",
      " 6351 5231]  Test : [2223 1071 1725 ... 1432 3611 4319]\n",
      "Train : [3042 6774 2853 6175 5551 4135 4040 1117 5005 5235 1811 4265 6256 6841\n",
      " 3372  993 2297 2249 5819 2847 2661 1446 3358 1359 4464  998 2017 6760\n",
      " 7074 2087   43 6261 2170 1537   93 1695 3010 1213 1382 2798 4557 6308\n",
      " 7091 6176 4942  509 5349 1025 3767 3831 3409 2691 2702  147 3087 6681\n",
      " 2296 2777 1858 7441 4226 5547 5083 2031  490 4500 3508 1778  997 2479\n",
      "  664 7260 6056 4420 6421 2288 4411 7175 6663 3259  258 7399 1755 5827\n",
      " 7185 3213 2921 7498 2729  788 4920 3055 6293 5455 7334 2917 5643 6925\n",
      " 6143 2448]  Test : [1997 5118 3207 ... 1552 3252 3550]\n",
      "Train : [1104 2535 4032 7122 4419 1786 4449 2435  219 4631 6449 4304 2634  651\n",
      " 1070 3420 2767 6525  380  174  309 3570 5369 5614 6392 3622 3585 6272\n",
      " 2122 7318  579 4342 1803 3596 2527 7252 4319 2924 3540 3265 4254 5329\n",
      " 3803 4624 5649 2159 6419 6177 2165 7215 2842 1268 6817 2659 3080 6084\n",
      " 7146 1686 2199 1073 5521 4056 4176 4160  907 7009 2850 3092 1958 5934\n",
      " 1549 3215 7022 5472 2728 7086 4707 4972 2445 2543 7050 2187 3820  146\n",
      " 3194 1315 7034 7329 1257 1875 7405 3212 4017  437 4107 5134  621 7349\n",
      " 2046 6253]  Test : [5411 6679 6400 ...  656 1678 6913]\n",
      "Train : [2930 3170 1253 1022 1340 4717 5170 1294 6046 4032 5399 7015 4779   22\n",
      " 4363  351 4857 4916 4262 5265  916 7163 2040 2668 3029 6835  882 1176\n",
      " 2444 5481 5650 6143 2046 6012 2038 5952 4676 3085 2446 5187 6157 4382\n",
      " 1244 3323  921 1060 6210  570 2921 6554 6725 5402 6088 4199   46 5748\n",
      "  345 3166 4565  507 7126 6447 6401 5846  648 6352 6382 3933 4890 7173\n",
      "  164 2889 2714 2311 2816 1529  160 5432 5897 1627 4094 3116 4988 4034\n",
      " 2662 2160 5309 6116 1168  620 6571 3283 1464 4546 1743 2625 7273 3152\n",
      " 2053 5661]  Test : [1728 7195 3019 ... 4750 3350 1017]\n",
      "Train : [3989 4181 5582 4317 4780 6852 1881 3128  396 3634 5942 1549 3742 5742\n",
      " 6850    2 2891 6970 1922 4794 3843 5225 4775 1003  905 5119 1597  606\n",
      " 3072 6089 7371 7471  797 3858 2160  415  527 6456 4262 6484 3835 6658\n",
      " 2973 6751 6436 5158 5454  954 6597 6636  107 6791 4138 2562 3159  936\n",
      " 4005 3410 1211 3602 1899 4427  450 1992 5135 1130 5845 3255 1915  211\n",
      " 5334 4682 5167 3347  908 4876 2267 1671 1386 1304 4555 4241 3967 1063\n",
      "  297 1306 6792 4952 2945 7176 5256  487 4316 2428 5452  548 6635 4299\n",
      " 5336 6919]  Test : [1262 3733 5412 ... 6841 5327 5882]\n",
      "Train : [1462 2925 2439 4245 3343 4336 6234 5059 2812  651 6927 5250 2190 1528\n",
      " 1174 2805 6250 2935  866 7051 5618 3664 3993  748 6939 1413 5398 4120\n",
      " 1355 2462 1977 3350 6764  411 1254  626 5381  425 2936 6486 5448  949\n",
      " 4862 4534 1353 1708   79  921   40 2594 2013 3594  713 5157 2948 4876\n",
      " 7089 3008 6634 4866 3796 7139 1953 5629 6605 6908   63  698 1654 5372\n",
      " 5970 2155 6826 6598 7367 6679 3070 3748 6780 6912 3272 2873   88 7038\n",
      " 4487 6244 5031 5580  268 6638 4229  147 7245 5759 3318 3518 2431 5221\n",
      " 6662 2700]  Test : [2903 5143  765 ... 5539 3938  174]\n",
      "Train : [7148 6903 3604 7354 7054 4232 1182 7063 5342 4204 6370 7113  965 6581\n",
      " 2886 4433 2318 5732 4401 5157 4223 3172 1792 5438 6971   95 3768 3456\n",
      " 2920  910 7147  349 1225 5932 1896 1729 2630 5665 5694 5895  743 2829\n",
      " 3102  904 6121  291 1004 6755 5122 4035 5459 5650 4752 5161  745 6465\n",
      " 4795 4251 7059 5430 5954 4813   75 4892 4805 6723  508 6372 6156  705\n",
      " 6728 2330 1881 3216 4240 2003 5900 4583 5466 1095 5276 4634 1890 6635\n",
      " 1507 2883 3109  225 1559  345 6480 5905 2340 4819 2948 3648 4931 1649\n",
      " 3431 3895]  Test : [7367 2947 3470 ... 1350 6337 1143]\n",
      "Train : [2988 6105 6376 1930 6146 5516 2679 3094 2393 4895 5092 1781 1362 5285\n",
      " 1033 1367  193 5559 6135  445  635 5522 4405 3782 6638 4305 3059 2258\n",
      " 4526 4588 6548 3450 3614 1980 2146 4823 1386 1401 1656 2309 4239 3878\n",
      " 1575 1609 2255 7010 5385 2546  827 7347 4442 2214 1711 6136 4770  433\n",
      " 3277 6408 1859 7422 5379 3744 5614  703 4006 4759 2553 5814  223 1169\n",
      " 1491 2499 4927 3418 5037 5164 6067 3320 3639 3688 2060  975 4836 2247\n",
      " 4887 4769 1683 4060 1254 6059 7262   93 5410 1561 1965 4589 3552 1969\n",
      " 4783 4192]  Test : [2609 2589 1657 ... 2092 3167 4562]\n",
      "Train : [1705 1414 1081 4105 7274 3074 3013  861 3667  858 1075 5116 1398 3881\n",
      " 4935 6094 1269 1083 4114 3459 4985 3179 5671 5761 1250 3034 4818 1914\n",
      " 6859 5269 2161 5134 1535 6886 5842 1403 2419 2506 2731 5734 2434  193\n",
      " 2242 3431 6316 5228 1848 5871  438 1360 1822 4728 5195 1652 5580 5698\n",
      " 5614  762 1300  544 5318  212 4267 7175  378 2695 6125 3945 5972 7375\n",
      " 6952 2073  520 2412 1575 2969 1056 7312 4491 7221 7489 4970  115  963\n",
      " 3570 1793 2699 5193 4437 3329 2542 7167 3229 5688 1803 5230  569 2492\n",
      " 4003 3461]  Test : [ 701 6743 5685 ... 4349  819 2933]\n",
      "Train : [2816 3411  523 2452 5677 5094 1072 6546 2699  676  670 1307 5737 7062\n",
      " 1842 4119 2344 7172 4104 3646  140 5832 5637 1315 1235 2002 2603 4552\n",
      " 1003 6836  894 5975 2674 4473 3095 1344 3860  990 4933 6678 2474 6990\n",
      " 2648 4736 4231 1013 5720 6341 2720 7069 6563 2846 2771 6973   20 2495\n",
      " 5299 5826 5165 6648  419   41 6124 3609  107 4626 6023  594 6745 7252\n",
      " 7057 5933 5019 4798 5227 2948 4462 1755 4288 3723 2731 3759 3191 7439\n",
      " 4996 4802 2985 1142 4854 6881 7341 1565 6626 4315 6615  473 1202 4765\n",
      " 4252 1321]  Test : [ 530 4456 2222 ... 4768 2431 4974]\n",
      "Train : [6998 2202  998 7423 6636 6426 7355 2004 4362 1414 6085  482 5971 4780\n",
      " 3407 2491  284  122 3345 2902 7161 6333 2230 2561 3789 3046 4215 7407\n",
      " 1918  339 2901 5799 3525  561  411 6627 6109 6562 6993 6458 2357 6899\n",
      "  272 5581 5773 6253  681 4273 4334 7475 2385 3048 6996 5649  478 6792\n",
      " 4841 4796 4290 6059 5898 2971 4303  483 5596 2878 2245 5235 1063 2498\n",
      " 1611 4966 7096 3114 2157  822 5551 6799 5628 6309 2592 4894 3568 5869\n",
      " 1963 3572 6155 3359 2533 1337 5789 4891 1974 2166 3946 2548 7283 5772\n",
      " 5610 6417]  Test : [  18  812 5991 ... 3376 5762 1424]\n",
      "Train : [4309 7275 2655 3380  253 6682 2298 6280 6809 2438 2002   49 4716 1528\n",
      " 4238    5 3078 5217 6185 6614 5607 6865 6822 2858  617  954 5871 4627\n",
      " 2507 3977 5333 3627 2678 1441  989 2479 1047 6196  863 7380 1469 4500\n",
      "  393 3581 2826 1986 1213 7001 7300 6433 2975  496 7243  786 3733 3046\n",
      " 6161 2785 6351 6583 2984 5719 3154 3979 4099 6921 2022 2284 6973 3569\n",
      " 5693  704 1729 5467 2970  755 3865 4924 6085 4495  472 3275 3917 5144\n",
      "  523 2714 5100  600 3624 5218 1201  969 6503 7358 6846 7329 4708 5351\n",
      "  759 4366]  Test : [7488 2424 5125 ... 6622 5946  383]\n",
      "Train : [ 949  456 7469 4595 3819 2230 5182 3178 1216 4781 1935 4432 5258  243\n",
      " 2999 2541  461 2549 4213 5209 5413 2698 7357 4241  605 4591 5198 3995\n",
      " 1478 1234 1308 3988 6794 3263 7139 2511  983 1354 4014 2638 4190 5951\n",
      " 1933 4996 5190 4963 6882  560 5264 4371 2908  714 5273  576 4979 1435\n",
      " 2558 7169  390 6532   19 5138 7131 3921 7204 2844 3548 1239 1117 5350\n",
      " 1263 1598 3002 1192  358 7328 4889 3218 5243 7114  361 3817 5466 6913\n",
      "  710 4387 1020  293 6750 2129 5063 3821 5465 7234 7304  193 3325  428\n",
      " 4129 4967]  Test : [6047 5179 6666 ... 5000 2980 3754]\n",
      "Train : [2083 4020 2368 5086 4519 2464 4009 3256 7394 5374 6855 1491 1205 3407\n",
      " 4372 1712 1717 3041 5284 2135 5356 6511 3851 5176  218 7366 1120 4612\n",
      " 1003  823 4785 2403  907 6824 4547 7299 5863 6185 3691 4563 3750 2189\n",
      " 6705 6405 5226 7165 3819  844 4047 6974 1627 4145 1449 2118 6007 6842\n",
      " 6804 4899 1597 3866  655 6569 2165 6984 2046 1293 6086 1129  211 2418\n",
      " 6526 3502 5065 3814 3046 3335  970 1818 5543 3835 4777 2727   90 4824\n",
      "  342 6934 4068 3605 2049 4897 5558 2953 2179 3520 1036 3607  232 3852\n",
      " 1363 1666]  Test : [3378 4080  163 ... 1775 2612 6436]\n",
      "Train : [1217 1500 3787 1220 5094 1248 7172  359 6855 2855 2857 7008 1013 7278\n",
      " 2406  421 7098 6448 1571  774 6878 6808 2956 4192  395 4644 1489 1226\n",
      " 2747 2436  911 3267 4014 7288 5950 2792 4446 7102 3534 6112 1560 6170\n",
      " 6351 1181 5862 1499 5410 3811  294 3266 6952 4429  345 6988 7456 5545\n",
      " 3763  425 3296 2567 5042 5438 3949 5371 5952 6579 3630 3281 3488  796\n",
      " 1255 7203 5008 7005 2433  789  112 3804  813 1872 4129 1529 3090 2983\n",
      " 1995 6340 7384 6089 3200 6338 5522 4706  299 1124 7422 1286 1579 4570\n",
      " 6371 1654]  Test : [1758 1844 3448 ...  661  487 7045]\n",
      "Train : [3998 5406  274  214 1633 4872 5768 7308 6715 5847 7129 1637 3371 3495\n",
      " 6267 5349 2685 2941   66 1756 4365 2382 6885 4881 5820 5282 1398 3751\n",
      " 6874  573 5969 7375 2380 5011 3928 1531 4210 1609 6032 6550 4783 4596\n",
      " 2825 6343 5908 5000 2421 4730 4434 5245 3801 4835 1548 1912 6391 7470\n",
      " 6120  205 5082 2896 1719 2444 2681 3736 5229 5529 1129  564 7075 4350\n",
      " 3777 3453   44 4371 2089 3176  304 6572 4310  830 3266  619 5083 5828\n",
      " 1246 3353 4535 4650 4068 4558 7272 5542 6224 3161 2052 5923 1718 6682\n",
      " 5247 4425]  Test : [6904 3959 3593 ...  513 2264 2333]\n",
      "Train : [6993 4231 6711 3662 3671 5292 4096 7332 1271 1996 6450 5278 4896 6236\n",
      " 4945  419 1889 5031 3008 7188 4400  318 5491  766 6848 5665 3359  622\n",
      " 4803 6220 2874  756 1045 7073 2472 2717 5418 3353 1909 2415 1462 4689\n",
      "  629 3673 5652 4178 2244 3380 1999 5702 3865 6371  328 3423 6466 6897\n",
      "  116 5943  609 6545 6134 2159 6237 6089 7340 2146 2670 1504 5319  330\n",
      " 4612 1455  861 4038  253 6927 6683 6028 5773 3170 1025 6994 5038 4190\n",
      " 6645 1372 1545 1633 1818 3937   63  763 5568 5247  639 1890 3496  263\n",
      " 2122 1154]  Test : [4420 5940  502 ...  479 2026 7306]\n",
      "Train : [2242 1484 5743 7238 5642 1765 6047 3140 3655 5200   12  628 1974 6071\n",
      " 7077 6540 3025 2438  204 2556 6051 4412 5022 3387  136 1139 7497 1618\n",
      " 6288  751 7253 3499   25 1699 5465   50 2610  475 6577 6841 6909 6366\n",
      " 5584 2964 1257 3837 2109 7111 5796 2731 6654 1197 2745 3697 3258 3922\n",
      " 3470 1150 2978 2824 3850 2989 4240 6283 1399 4680 6584 3618  947 1230\n",
      " 6915 3979 5070 1181 1313 5598 7289 3101 6143 5729 1817 7133 2612 3536\n",
      " 7414 5637  903 1797 1882 7260 1553 6222 1027 5257 1280 3458 5735 3421\n",
      " 1546 6249]  Test : [ 644 2310 3603 ... 4274 6539 3407]\n",
      "Train : [6858 2363 1185 6462 2977 2869 7091 6989 3330 4417 2068 2061  424 2512\n",
      " 3401  462 1107  218 3058 6598 4485 5330 3122 5943 4707 1432 6255 7435\n",
      " 2045 7368 2199 2403 5630 3968 6515 2829 4356 6879 6587  490  716 4387\n",
      " 2731 7125 4748 3132  640 3015 1377 7372 3288 3230 2181 1082 2835 3141\n",
      " 1941 5200 3529 5313 3596 3752 4159 2023 4761 2537 7170 4242 3911 7212\n",
      " 3560 4557 3391 5318 4537 1818 3215 2999 6483 2310 1854 3461 5850 3415\n",
      " 4924 6425 2975 2240 1460 6573 2828 2682 5274 1310 4515 4310  855 2674\n",
      " 1822 3092]  Test : [5551 5957 5259 ... 1974 4512 3392]\n",
      "Train : [5352 3439 4997  410 2440 6426 2131 7310 7226 1533 2664 1039 7063 5395\n",
      " 2116 3376 1576 5581 6348 6427 5088 2347 4816 3683 1717 6402 1119 5163\n",
      " 3943 7478 2593 6213  605 6562 2747 3631  963  948 4228 6809 6172 3132\n",
      " 4184 1887 1145 3133 3538 3302 3370 1067 4272 4156  974 3517 3821 4432\n",
      " 4623 2905 4239 2739 1525  556 5704 5871 5674 6252  608 1665 2146 1238\n",
      " 5278 1513 6897 1362 5569 2123  859 3324 5886 4304 4840 2437 7460 6524\n",
      "  502 6029 1965 6346 5910  312 2324 4117 2215 3269 2959 3018 1392 6100\n",
      " 6949 1033]  Test : [7485 1683  437 ... 5812 5215 2892]\n",
      "Train : [5275 2581 2676 5923 6599 7033 2690 7192 6267 2435 2787 5882 3678 2948\n",
      " 6086 5049 7180 6427 2483 4278 1493 1643 1878 3412 2726 4975 5113  544\n",
      " 4794 4946 2084 2108 3237 1253 3310 1662 2147 1531 2928 4487 5877 6886\n",
      " 5863 5571 6075 1614  149 3180  582 4080 5438  183 4862 7151 2661  500\n",
      " 7375 2499 5472 1981 3912 5529 7285 3494 5926 1170 6497  156 7481 3268\n",
      " 2965 5161 5656 6753  929 2391 3354 2051 1763 5860 6474 3643 6550 3819\n",
      "   85  817 4297 4690 4401 4365 6249 5111 1051 2240 1039 6327 3950 3656\n",
      " 1078 7247]  Test : [2901 1331 4595 ... 3253  865 6077]\n",
      "Train : [1517  885 1930  259 2354 1861 1670 4809 5909 3977 2926  616 4347 5473\n",
      " 2457 6548   65 5715 4526  458 6010 5227 3672 6124 3382 4122   39 7485\n",
      " 2734 7041 4502 5704 1535 5951 1237 6316 5095  468 4274   44 1366 3374\n",
      " 2183 4064 7486 2014 4942 4704 6064 4172 6849 6690 1700 6897 2356 3074\n",
      " 2170 5198 5734 1018 5709  475 3421  652 6227 2442 6998 7494 3141 3590\n",
      " 4427 1780 4795 5483 5274 6359 2766 6298 4731 4210 6161  771 3378 1505\n",
      "  427 1379  367 3775 5487  340 5795 6632 4152 4229 6881 5528 7031 3945\n",
      " 2682 3621]  Test : [6353 7153 5341 ...  311 6520 6851]\n",
      "Train : [3746 7371 4519 2567  325 6497 1713 6188 6836 3338 7033 2563 3867  585\n",
      " 3923 1109 3719 1424 1953 5962 6185 5493 7479 1044 7082 6994 3313 4770\n",
      " 4560 5415 4777 5146 3622 4062  319 2128 6516 3364 2972  412 6273 7165\n",
      " 6455 3331 5595  710 6514 3027 2186 3672  899 1660 2127 4774 3025  322\n",
      " 2989 4614 5745 5728 2052 2357 5495 6088 1814 2279 2595 2093 2629 3966\n",
      " 5732 3320 1392 1522  984 3151 4747 5900 3219 3451  390 2585 4065  572\n",
      "  926 2394 4526 3546 6622 2297 6912 2761 5045 3993 2970 3276 4063 6520\n",
      " 5476 6314]  Test : [  79 5899 7144 ... 4635 2131 3465]\n",
      "Train : [1064  474  233 5981 1971 5249 5374 1310 6569 2475 3847 2532 3659 4602\n",
      " 1121 3152  145 6916 5223  390 7234 4200 4436 2779  399  898 7180 4388\n",
      " 5205 7394 2832 1369  944 6526 5226 3757  733 2128 2719 4457 5909 1448\n",
      " 6903 4275 1785 4841 2170  640 3713 6456 5271 2177 7280 2575 7064 1248\n",
      " 4614 5555    2 6861 3586 4105 4509 3404 4396  953   60  361 3666  846\n",
      " 5122 3904 7485 1665 5690 6981 2941 5066 7027 2760 1157 1833 1786 3882\n",
      " 2773 5799 1294  796 1442 6814 5528 2068 3036 1998 6448 3839 4184 6389\n",
      " 3674 2472]  Test : [1578 5614 5937 ... 5987 6372 5817]\n",
      "Train : [3771 1429 3328 1536 1182 3168 6819  112 7267 6209 2962 5865 1881 2642\n",
      " 5498 3723 5090 5524 7450 5657 1410 5455 6054 3703 3661 2714  389 3316\n",
      " 4018 5749 7399 1000 3207 4448 2100 1652 1522 5487  932 1247 3785  336\n",
      " 6680 1683  982 3548 4311 2193 1904 5187 3953 5831 5392  965 4259 4547\n",
      " 4307 5552 1949 3993 7277 4126 6553 1936 5950 6825 1890 2827 7367 4863\n",
      " 1352 5537  765 4225 7442 4237  244 7181 3877  461   84 3913 5291 5015\n",
      " 5283 6250 6425 2743  155 4783 2407 1274 3017 3365 6756 5840 4370 7235\n",
      " 3014 1080]  Test : [1675   95 4336 ...   89 5543 2058]\n",
      "Train : [1416  452 1465 4283 6663 2307 4411 6386 6610 1159  627 5953 7194 3419\n",
      " 6829 3595 1706 1067 1541 7030 4025 3079 1926 3059 6011  429 1012 6331\n",
      " 4681 1873 7352 3115 1284 3078 3285  891 4110 1171 7364 4200 1483 2333\n",
      " 3080 4905 1222 3765 5998 1643 3163 7158 5391 2181  201 6924 5879 5373\n",
      " 2726 6197 1713  401 1252 2234 5602 5896 5311 6621 5035 1903 5543  364\n",
      " 5678 4790 3962 5021 5049  222  662 5587 1872  936 4391 1149 5297  428\n",
      " 6453 6268 6219 2207 5471 5063 4469 7416  458 3250 6638 3877 2379  110\n",
      " 1313 2558]  Test : [3368 2080 5798 ...  640 6975 5259]\n",
      "Train : [1797 1927 1878 2929 2160 5528  708 2307 6099 2790 4901 7150 2934 7281\n",
      " 4232 3878 5368 6557 2313   76 2679 6078 3543 3851   19 6678 1910 1333\n",
      " 1740 3250 4650 3193  658  903 5092 2080 1088 6376 6064 4304  570 6115\n",
      " 3973 5312  887 3797 5461 4338 7190 6020 7241 7429 4225 6811 5263 1689\n",
      " 7354 6717 4710 1884 3496 3522  584 3111 2078 5657 2192 6799 3095 6383\n",
      " 1895  275 1583 6791 7056 5781 5005 4634 6191 3585 5979 1085 6892 2114\n",
      " 3688 1992 2474 1059 2992  746 6547 5706 4062 7148 5642 3138 6199 4764\n",
      " 4693 6873]  Test : [6160 2901 3755 ... 6866 2611 5614]\n",
      "Train : [1857 5236  839 3650 3581 6184 4661 6266 5832 3728 4259 2094  615 7363\n",
      " 7372 6653 2612 5193 2434 3184 4356 4408 3847 7307 4583 1649 4287 6084\n",
      "  333 4241  738 5379 6158 4643 4008 5853 5931 1226 2147 3011 4376 1306\n",
      "  288 5153 3176 2446 3330  972 4433 5163 4935 1748 7201  173 2564  474\n",
      " 4058 7403  393 3285  879 3126 4527 3768 3305 1600 1764 3457 6435 2123\n",
      " 6684 5295  701 5897 4746 4592  942 3160 7498 1020 1171  455 5659 5840\n",
      " 2113 5962 2586 5474 2883 4321 2865 6978 2812 7430  453 7163 6174 5035\n",
      " 6945 6573]  Test : [ 322 1769 2033 ... 1807 5560  566]\n"
     ]
    }
   ],
   "source": [
    "#Let's see how the shuffle split works:\n",
    "for train_index,test_index in rs.split(X_train):\n",
    "    print(f'Train : {train_index}  Test : {test_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we want to use those indexes as masks on the training set and then save those mini subsets\n",
    "\n",
    "subsets = []\n",
    "for train_index, test_index in rs.split(X_train):\n",
    "    subsets.append([X_train[train_index],y_train[train_index]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Instances : [[-6.31133500e-02  2.54608017e-01]\n [ 6.74796497e-01  6.40993741e-01]\n [ 8.55144319e-01 -9.08019262e-01]\n [ 2.14973498e+00 -7.36523899e-01]\n [ 7.62969473e-02  9.99801356e-01]\n [ 1.71371352e+00  3.91760997e-03]\n [ 9.72627865e-01 -1.36327132e+00]\n [ 8.45395321e-01 -1.72215914e-01]\n [-2.78699593e-01  1.68340402e-01]\n [-7.13836932e-01  2.44085491e-01]\n [ 9.01435905e-01  9.32643505e-01]\n [ 7.18722991e-01  1.48512173e+00]\n [ 1.11136765e+00  1.39499363e+00]\n [ 1.48802103e+00  4.61411610e-01]\n [ 1.12797620e+00  2.73833566e-02]\n [-2.52602418e-01  3.03876257e-01]\n [ 1.33328605e+00 -4.11302660e-01]\n [ 2.33243800e+00  1.81983046e-04]\n [-6.15051236e-01 -2.17198902e-01]\n [ 1.05312941e-01  1.31028847e+00]\n [ 3.53143944e-01  4.77547662e-01]\n [ 2.16977686e+00  4.81907079e-01]\n [ 1.49213910e+00  2.84530025e-01]\n [-1.45785937e+00  5.93821016e-01]\n [ 8.13821872e-01  1.13291121e-01]\n [ 4.94138926e-01  3.76153568e-02]\n [ 8.77065347e-01 -5.32788565e-01]\n [ 7.56773998e-01 -2.83673222e-01]\n [ 4.50869194e-01 -1.54824750e-01]\n [ 9.73308010e-01 -3.93098901e-01]\n [-1.00994551e-01 -5.56575139e-01]\n [-1.02454925e+00 -4.92946599e-01]\n [ 1.56881622e+00 -4.11839935e-03]\n [-2.85685027e-01  1.66173141e+00]\n [ 5.72713027e-02  1.22060224e+00]\n [ 8.08500813e-01  7.37577456e-02]\n [ 5.14192659e-02 -5.33597450e-01]\n [ 1.78480445e+00 -2.06901783e-01]\n [-6.44469700e-01  9.02955909e-01]\n [ 9.79118934e-01  1.19628986e+00]\n [ 1.47583002e+00  1.60595015e-01]\n [ 1.14706856e+00  6.13719308e-01]\n [ 3.37448596e-01  1.77039251e-02]\n [-8.65677735e-02  1.96158327e+00]\n [ 2.37616764e+00 -5.55912960e-01]\n [ 2.24025084e+00  6.51419867e-01]\n [-1.14561910e+00  5.24958492e-01]\n [-1.90130388e-01 -1.26937842e+00]\n [ 1.30643068e+00 -4.29656678e-01]\n [ 7.96240690e-02 -1.04190680e+00]\n [ 1.25294499e+00  8.83432768e-02]\n [-8.38400229e-01  1.27099040e+00]\n [-6.13662113e-01  1.03147202e+00]\n [ 6.47137149e-01  7.70226713e-02]\n [ 2.52327235e-01  7.37598683e-01]\n [ 2.08550154e+00  3.21085805e-01]\n [-1.43690630e+00 -9.87435068e-02]\n [-1.85638049e-01  3.87694016e-01]\n [-3.48436113e-01  8.63168127e-01]\n [ 2.58560497e-01  5.98518935e-01]\n [-7.34126202e-01  4.95975269e-01]\n [ 1.13388844e+00  2.99902940e-01]\n [ 1.98232325e-01  4.70952335e-01]\n [ 1.01437440e+00  1.70620181e-01]\n [ 1.20868206e-01 -8.15628735e-01]\n [ 8.33449757e-01 -6.14628241e-01]\n [ 7.40663973e-01  5.76568359e-01]\n [ 1.19305838e-01  6.02060973e-01]\n [ 4.02840460e-01  1.02170321e+00]\n [ 1.26377764e+00  7.22488205e-01]\n [-6.73062378e-01  2.45822831e-01]\n [-9.38144498e-01  1.27307746e-01]\n [ 1.70385232e+00 -6.59052002e-02]\n [-5.35177608e-01 -3.93837171e-01]\n [ 5.10441841e-01  6.00009732e-01]\n [ 7.93612831e-01 -8.74297024e-01]\n [-7.27250164e-01  1.78816882e+00]\n [ 6.58781249e-01  5.18081698e-01]\n [-1.27492040e+00  1.14808873e+00]\n [ 2.33453136e+00 -6.76657127e-01]\n [-3.39659397e-01  1.24591807e-01]\n [ 1.20139064e+00 -4.33654549e-01]\n [ 1.41585820e-01  1.15855980e+00]\n [-4.12994410e-01  1.69449122e-01]\n [ 8.49896784e-01  1.08071755e+00]\n [-9.10080907e-01  9.78325918e-01]\n [ 1.59666205e+00  9.05565999e-01]\n [ 1.70713401e+00 -4.45375244e-01]\n [ 5.97271558e-01  1.08043564e+00]\n [-8.33757109e-01  1.05280496e+00]\n [ 1.92908334e+00  1.14522767e+00]\n [ 5.46290596e-01  1.50411624e+00]\n [ 1.18219515e+00  7.03274176e-01]\n [ 8.22599830e-01 -1.85988281e-01]\n [ 6.01406282e-01  2.37201309e-01]\n [-1.81686010e+00 -5.18353988e-01]\n [ 1.41641456e-01  7.52340907e-01]\n [ 3.91091698e-01  1.00304652e+00]\n [ 1.48047766e+00  3.69803331e-01]\n [-9.34869230e-01  3.80848387e-01]]\nLabels : [0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1\n 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1\n 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "#Let's take a look at the structure of each subset\n",
    "print(f'Instances : {subsets[0][0]}')\n",
    "print(f'Labels : {subsets[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now will train 1000 decision trees on these subsets using the parameters provided by the grid search\n",
    "accuracy_scores = []\n",
    "for subset in subsets:\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.set_params(**grid_search.best_params_)\n",
    "    tree.fit(subset[0],subset[1])\n",
    "    prediction = tree.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test,prediction))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.799048"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "#Computing the mean of the accuracy scores:\n",
    "np.mean(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each instance of the test set generate the predictions of 1000 Decision Trees and keep the mode of the predictions\n",
    "#We may want to have all our trees in a forest:\n",
    "forest = []\n",
    "for subset in subsets:\n",
    "    single_tree = DecisionTreeClassifier()\n",
    "    single_tree.set_params(**grid_search.best_params_)\n",
    "    single_tree.fit(subset[0],subset[1])\n",
    "\n",
    "    forest.append(single_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have a forest, we can generate the predictions:\n",
    "from scipy.stats import mode\n",
    "\n",
    "predictions=[]\n",
    "temp_pred = []\n",
    "for instance in X_test:\n",
    "    for tree in forest:\n",
    "        \n",
    "        temp_pred.append(tree.predict(instance.reshape(1,-1)))\n",
    "    predictions.append(mode(temp_pred)[0])\n",
    "    temp_pred.clear()\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "The previous method is not recommendable. I did it because I didn't want to copy the method used in the book"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions).reshape([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8496"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "#Finally let's test the accuracy of the predictions\n",
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "source": [
    "5% more accuracy is not bad at all"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}